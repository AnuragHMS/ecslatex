\section{Amazon Web Services}
\subsection{What is AWS?}
Amazon Web Services (AWS) is the one of the largest cloud-computing resources in the modern day. With many large companies using AWS as a way of bringing data and content to end-users from a server back-end on as grand a scale as necessary. On the main AWS website the following quote is shown.
\begin{quote}
\textit{Amazon Web Services (AWS) is the world’s most comprehensive and broadly adopted cloud platform, offering over 175 fully featured services from data centers globally. Millions of customers—including the fastest-growing startups, largest enterprises, and leading government agencies—are using AWS to lower costs, become more agile, and innovate faster.}  \cite{ch1_2_what_is_aws} 
\end{quote}

The above text gives an idea of the grand-scale that their services cater for with many millions being provided for. With data centres located in many regions across the globe with their own sub-regions or Availability Zones as will be discussed later. Some major companies that use AWS include \textit{Netflix}, \textit{Facebook}, \textit{LinkedIn}, and \textit{F1 Insights} to name a few. They all use AWS for content-delivery, data storage of users and website files, security and authentication and to generally manage routing of users to reduce server loads within and across regions. There also exists many abilities within AWS for disaster management (either physical or virtual) and it provides recovery tools them to ensure the end-user does not go without their service. This proof is discussed in the \textit{Gartner Report} \cite{ch1_2_amazon_is_best} that shows AWS as the best Cloud-based service for ability to deliver and completeness of services provided and gained for.

As a result of the accessibility to learn and setup the various services provided by AWS, large companies, start-ups, and research projects are able to flourish. Some of the most common services are \textit{EC2}, \textit{Lambda}, \textit{Simple Storage Services} (referred to as \textbf{S3} for the rest of the report), API Gateway, and Sagemaker to name a few of the many services. The services named here will be used within the GDP and discussed in the later sections. 

To access these AWS services, Identity and Access Management (IAM) roles are usually created first. This step can be done by accessing the IAM dashboard, which is also an AWS service itself. Each IAM role created is attached with an editable permission policy which allows users to define permissions to make AWS service requests. 

Research into other possible services and architectures were also conducted to handle the project. In the following few sections, the report will discuss case studies for how AWS has been used for IoT data collection, processing and Machine-learning environments as well as Edge-Capabilities within the industry. After this, research into the specific services and how they can be used will be discussed.

\subsection{AWS Case Studies}

The following case study takes a look at using IoT Core, which will later be used for deploying data extraction, transformation and loading functions as well for taking advantage of localised ML Models. IoT Core and all of its services can be deployed using the IoT Greengrass services, which is discussed later.

\subsubsection{Bayer Crop Science - Using IoT Core}
Bayer Crop Science used AWS to help gather real-time agriculture data from farms and to provide real-time analysis for usage by analysts, and for farmers to be able to react to changes within the environments \cite{ch1_2_case_study_1}. Initially data was collected manually from various sensors across the farm, but collating this data and analysis would take time and would eventually lead to issues not being resolved or being resolved too slowly. These issues could arise if seed deposition seized, water and feed to the crops was not being delivered correctly, harvested sections were not being correctly collected etc... With a whole host of consecutive problems that could come up if not dealt with immediately, mainly leading to reduction in harvest quantity, quality of produce and profit for the farms.

Bayer Crop Science were able to leverage AWS IoT Core, AWS Greengrass and AWS IoT Analytics to get seed data during planting and harvesting to analysts within minutes rather than days and give the farmers ample time to respond to the data and to take the actions needed. This is a case where AWS can speed up data ingestion and analysis to minutes and seconds rather than days and hours. The analysts have easy access to the data anywhere from the globe and can provide feedback to the farmers immediately. However, this solution does not fit into the category of Industry 4.0 where Machine Learning (ML) is able to assist in making decisions. In this case study AWS is only leveraged for data collection and display for an analyst to make decisions. Our project aims to further the work done by Bayer Crop Science by involving ML technologies into the system, but also to localise the ML Model. In this case, the ML Model local to the farms would be able to provide feedback in seconds rather waiting than for an analyst to analyse the hundreds if not thousands of sensor data collected depending on the scale of the farms.

\subsubsection{Sensor Networks - Predictive Analysis}
Sensor networks is an IoT Tech startup that focuses on building a platform to monitor and help home insurance companies. The company places hardware sensors onto home appliances and are able to monitor performance and to detect and minimise risk to possible breakdowns within the household \cite{ch1_2_case_study_2}. With each monitoring device connected using IoT cloud, Sensor Networks are able to collate the data into the cloud and use predictive analytics to inform home owners of any issues and possible risks and also to provide evidence for insurance claims. 

The usage of machine learning in this case study is for predictive analysis and requires large collation of data, but the processing still happens on the cloud-side. They use AWS Sagemaker, which is discussed in the later ML Technologies section to produce new models and deploy them quickly to the production stage. However, the company did not deploy the models locally for the lowest-latency response. For this company there was no need to provide analysis of the system immediately, hence it was avoided. The case study instead looks at the integration of IoT core to be able to pull data from many sensors around the household. Collate them into storage and then use the data for training and verification of models. AWS IoT core allows for all of this to happen, as well as for IoT analytics to analyse and display the data in real-time, all while supporting model deployment.

\subsection{AWS Services}
The following sections give a brief intro to the AWS Services that we plan to use throughout the project.

\subsubsection{Simple Storage Services - S3}
In order to first fully understand the benefits of S3, the types of storage must be understood. This will allow the reader to know why S3 was chosen as the primary storage utility.
\paragraph{Block Storage}
The data is separable into small blocks. (i.e. A database), in this case there might be 200 entries within this database file. Block storage allows for users to change only 1 entry and keeps the read/write requests very small for each access periods.
\paragraph{Object Storage}
Object storage allows for entire file(s) to be added, removed and read from and any small modification to a single file would result in a complete re-upload of that file. 

S3 is used for WORM operations (Write Once, Read Many) - which is very useful for website resources that do not change often. It is also good for storage of unpredictable sizes due to its vertical scalability. Since the project does not require us to send SQL data or db files around, using S3 is a clear choice, where each object to be stored is a data file optimised for transport in the telemetry pipeline. 

S3 allows for storage of objects in \textit{buckets} where each bucket \textbf{must} have a unique name in the entire region. There are several naming rules that must also be followed:
\begin{itemize}
    \item No uppercase
    \item No underscores
    \item 3-63 characters long
    \item Must start with lowercase letter or number
\end{itemize}
Once the bucket has been created, objects can be stored into it. Each object has a path associated with it known as its \textit{key}.

\begin{itemize}
    \item S3 Bucket Name: s3://my-bucket/
    \item With key: s3://my-bucket/\textbf{my\_file.txt}
    \item Object in sub-directory: s3://my-bucket/\textbf{my-folder1}/\textit{my\_file.txt}
\end{itemize}

This is the basics of setting up an S3 Bucket. The user-interface allows for easy creation and access to S3 and the setup of the buckets used is discussed further during the Implementation Section (\ref{Chapter:Implementation}). S3 also allows for object versioning within the S3 bucket to ensure data is never lost by being overwritten. This feature is not useful to us as S3 is being used to store new data files as opposed to re-writing the same file over and over. S3 also has encryption which can be done in three different ways, from server-side (SSE), client-to-server (SSE-KMS) or completely client-side (SSE-C). More details can be found in the AWS reference documentation \cite{ch1_2_s3_encryption}. 

\subsubsection{Lambda - Serverless Functions}
AWS Lambda is one of the most powerful features of AWS. Lambda has the capability for the user to deploy code and for it to spin up as many run-time instances as necessary for the amount of requests to use that function. It is defined as \textit{Serverless Computation}. Where the \textit{Serverless} feature means that the cloud provider, or AWS, is in charge of the server back-end and does not need to be managed by the user. This means writing and deploying the code is all the user needs to worry about. 

Lambda can be setup in various ways depending on the requirements, with the main ability to be triggered on various events that occur around the AWS Cloud. For instance for every new file uploaded to S3, the Lambda trigger could be setup to handle that new data. Or Lambda could be a function that runs when called from an API Request. What exactly this entails is discussed in the later exploration of API Gateway. 

The following programming languages are supported by Lambda for deployment, along with several previous versions of these languages:
\begin{itemize}
    \item .NET Core 3.1 (C\# / Powershell)
    \item Go 1.x
    \item Java 11 (Coretto)
    \item Node.js 12.x
    \item Python 3.8
    \item Ruby 2.7
    \item Provide your own bootstrap on Custom Entity
\end{itemize}
Lambda is a great resource that lets users do ETL (Extract, Transform and Load) operations on datasets or to respond to requests, which is why it is so useful for the project (i.e. for every new dataset placed into an S3 bucket, Lambda can be triggered automatically to process the data. This is explored later in the Design and Implementation sections).

\subsubsection{EC2 - Elastic Compute Cloud}
The EC2 instance is the ability to own a computing resource where you can set how much processing power you need, the amount of RAM, the OS running on the instance, and the amount of network or local storage available to the device. Each EC2 instance can be setup to run commands on startup and instances can be configured to accommodate for more users trying to access a service via the usage of Automatic Load Balancers, and horizontal scalability within EC2. Horizontal scalability is the ability to deploy new instances with the same setup immediately (versus vertical scalability which would increase the specifications of the individual instance).

\subsubsection{API Gateway - An entry point to Cloud Features}
API Gateway allows developers to setup a RESTful framework \cite{ch1_2_api_gateway} to allow for an entry point from outside of the cloud to services or resources within the cloud securely. Authentication and authorisation can be setup in order to ensure only allowed users access the resources. API Gateway also allows users to send data to Lambda functions by packing the data into the body of the HTTP(s) request  and ensuring SSL/TLS (security) certificates are present if necessary.

\subsection{IoT Core - Deploying AWS Services}
IoT Core is a set of services that allows deployment of AWS Cloud Services to featured devices. Using IoT FreeRTOS, IoT SiteWise or IoT Greengrass allows various connections to the Cloud and localisation of services to the device. Data can then be gathered by these various features into the cloud and data analysis can be performed. It is a streamlined service making it easy for large scale operations with many IoT devices out in the field or industrial work place to collect and view data. IoT Greengrass and deployment service of various Cloud services is one of the key components for this project and is explored further in the later Edge Technologies section \ref{edge_node_technologies}.



\subsubsection{Sagemaker - All things Machine Learning} 
AWS Sagemaker is a machine learning service that provides an IDE to create  a notebook instance where one can write and execute codes in Jupyter notebooks. It allows users to build and train machine learning models. Besides this, built models can be deployed and their performance for prediction can be monitored. These processes can be carried out by using the Sagemaker Python SDK or the Sagemaker SDK for Python (Boto3).

AWS Sagemaker allows access to the AWS cloud storage services such as S3 and EC2 to import datasets to train the machine learning model. An execution role is defined in the script where it grants the user permission access to datasets in cloud storage services.

Moreover, AWS Sagemaker contains several built-in machine learning algorithms for supervised learning, unsupervised learning, computer vision and natural language processing (NLP). Examples of supervised learning algorithms include:
\begin{itemize}
    \item XGBoost
    \item k-Nearest Neighbour (kNN)
    \item Factorization Machine (FM)
\end{itemize}
Examples of unsupervised learning algorithms include:
\begin{itemize}
    \item Principle Compononent Analysis (PCA)
    \item k-Means Clustering
\end{itemize}
Examples of computer vision algorithms include:
\begin{itemize}
    \item Image classification
    \item Object detection
\end{itemize}
Examples of natural language processing (NLP) algorithms include:
\begin{itemize}
    \item Neural Topic Model
    \item Latent Dirichlet allocation (LDA)
\end{itemize}
The trained machine learning model can be deployed by creating an endpoint for it. This model endpoint can then be invoked and deployed somewhere in the cloud. For example, an AWS Sagemaker model endpoint can be invoked using Amazon API Gateway and AWS Lambda. Firstly, a Lambda function which calls the model endpoint is created. An IAM execution role that includes a policy is chosen to give permission to the Lambda function to invoke a model endpoint \cite{engdahl_2008}. API Gateway triggers an event that invokes the Lambda function where it will pass provided datasets through it. Therefore, an API gateway will be created and deployed for the specific Lambda function. The testing phase comes in by using Postman, an HTTP client for test services. An invoke URL is entered onto Postman alongside with the test data which then displays the prediction results from the machine learning model.

